VideoRAG Core - Service Architecture
===================================

1. High-Level Overview
-----------------------

This system ingests live or file-based video streams, analyzes each frame with multiple vision services
(detections, captions, OCR), aggregates the per-frame results, groups frames into temporal video segments,
stores each segment on disk, and indexes the segment summaries so they can be queried later using RAG
(Retrieval-Augmented Generation).

All real-time communication between processing services is done using Kafka topics. Two logical
microservices are responsible for long-lived storage and search over segments:

  - Segment microservice (SegmenetAgent): builds and persists video segments from per-frame analytics.
  - Indexer microservice (Indexr / RealTimeRAG): indexes segment summaries into Qdrant and serves queries.

Below, "service" means an independently running process (usually a container). Services are designed so
that they can be deployed as isolated microservices.


2. Kafka Topics and Data Flow
-----------------------------

The main Kafka topics and their roles are:

  - ingestion:      Raw frames from cameras (produced by the Stream service).
  - analyze:        Normalized frame messages for analysis (produced by IngestionService).
  - aggregate:      Per-service analysis results for each frame (produced by Captioner/Detections/OCR).
  - segmenet:       Aggregated per-frame analytics, ready to be grouped into segments
                    (produced by AggregateService, consumed by the Segment microservice).

High-level flow:

  1. Stream service publishes frames -> ingestion
  2. IngestionService normalizes messages and resize this frames  -> analyze
  3. Analyzer services (DetectionsService, OCRService, CaptionerService) read analyze and publish -> aggregate
  4. AggregateService combines all analyzer outputs and publishes per-frame results -> segmenet
  5. Segment microservice (SegmenetAgent) consumes segmenet, forms segments, writes them to disk, and
     calls Indexr to index segment summaries.
  6. Indexr microservice exposes RAG-style querying over stored segments.


3. Stream Service (Camera Ingestion)
------------------------------------

Files:
  - Core/services/stream/StreamManager.py
  - Core/services/stream/Stream.py

Purpose:
  - Manages one or more camera sources (RTSP or test video files).
  - Captures frames from each camera, resizes them, encodes them, and sends them to Kafka.

Key components:

  - Camera (dataclass)
      Fields: id, description, url, use_test_video, test_video_path, frame_size, fps.

  - CameraWorker (thread, in Stream.py)
      - One worker per camera.
      - Opens a video source (RTSP URL or test video file).
      - Reads frames in a loop.
      - Encodes frame as JPEG and then base64 (via Core.common.frame_utils.encode_base64).
      - Builds Kafka message:
            {
              "camera_id": <id>,
              "camera_description": <string>,
              "frame_id": <uuid>,
              "frame_time": <ISO timestamp>,
              "frame": <base64 JPEG>
            }
      - Sends messages to Kafka topic ingestion using KafkaProducer.

  - StreamManager
      - Manages the list of cameras and workers.
      - add_camera(camera_info: dict): creates Camera and starts its CameraWorker.
      - Periodically monitors worker threads and restarts them if they stop.

Input / Output:
  - Input: RTSP URLs or file-based test videos.
  - Output: Kafka messages on topic ingestion.


4. IngestionService
-------------------

File:
  - Core/services/ingestion/main.py

Purpose:
  - Acts as a gateway between raw stream messages (from the Stream service) and analysis services.
  - Normalizes messages to a common schema and forwards them to the analyze topic.

Key behavior:

  - Consumes from Kafka topic ingestion.
  - Optionally resizes the frame.
  - transform_message(msg): extracts and standardizes fields:
        frame_id, camera_id, camera_description, frame_time, frame (base64)
    plus any extra metadata, which is stored under an "extra" field.
  - Produces normalized messages to topic analyze.

Input / Output:
  - Input topic: ingestion
  - Output topic: analyze


5. BaseAnalyzerService and Analyzer Microservices
-------------------------------------------------

Files:
  - Core/services/base_service.py
  - Core/services/detections/main.py, Core/services/detections/Detections.py
  - Core/services/ocr/main.py, Core/services/ocr/OCR.py
  - Core/services/captioner/main.py, Core/services/captioner/Captioner.py

Common base:

  - BaseAnalyzerService
      - Subscribes to the analyze topic using a consumer group specific to the service.
      - For each frame message:
          - Calls analyze_frame(frame_msg) (implemented in subclass).
          - Wraps the result into a standard envelope:
                {
                  "frame_id": ...,
                  "camera_id": ...,
                  "timestamp": ...,
                  "extra": <copied from input>,
                  "service": <service_name>,
                  "result": <analyzer-specific result>,
                  "frame": <base64 frame>
                }
          - Publishes this envelope to topic aggregate.

      - ANALYZE_TOPIC: "analyze"
      - AGGREGATE_TOPIC: "aggregate"

Analyzer services:

  5.1 DetectionsService
      - Files:
          - Core/services/detections/main.py
          - Core/services/detections/Detections.py
      - Service name: "detection" (used as the service field in messages).
      - Model: YOLO-based detector (Ultralytics YOLO) for object detection.
      - Responsibilities:
          - Decode base64 frame -> image.
          - Run YOLO inference to get bounding boxes, classes, confidences.
          - Build a list of detections and a scene graph via SceneGraphBuilder.
          - Return per-frame structured result:
                {
                  "detections": [
                    {"class_id", "class_name", "confidence", "bbox", "width", "height"},
                    ...
                  ],
                  "relations": {
                    "objects": [...],
                    "relations": [...]
                  }
                }

  5.2 OCRService
      - Files:
          - Core/services/ocr/main.py
          - Core/services/ocr/OCR.py
      - Service name: "ocr".
      - Model: EasyOCR.
      - Responsibilities:
          - Decode base64 frame -> image.
          - Run OCR on the image.
          - Filter detections by confidence and uniqueness.
          - Return per-frame text information as a list of detected text strings.

  5.3 CaptionerService
      - Files:
          - Core/services/captioner/main.py
          - Core/services/captioner/Captioner.py
      - Service name: "caption".
      - Model: BLIP (Salesforce/blip-image-captioning-large).
      - Responsibilities:
          - Decode base64 frame -> image.
          - Generate image captions using BLIP.
          - Return one or more natural-language captions describing the frame.

Input / Output for all analyzers:
  - Input topic: analyze
  - Output topic: aggregate

Each analyzer can be deployed as an independent microservice that shares only Kafka topics
with the rest of the system.


6. AggregateService
-------------------

File:
  - Core/services/aggregate/main.py

Purpose:
  - Merges results from all analyzer microservices into a single per-frame record.
  - Ensures that detection, caption, and OCR results are all available before emitting.

Key concepts:

  - Topic: aggregate (input)
  - Topic: segmenet (output)
  - REQUIRED_SERVICES = {"detection", "caption", "ocr"}

Behavior:

  - Maintains an in-memory dictionary keyed by frame_id, storing:
        - meta: camera_id, timestamp, extra
        - services: { service_name -> result }
        - frame: base64 image
        - last_update: timestamp

  - For each incoming message on aggregate:
      - _update_frame_state(msg): updates the in-memory state for the given frame_id.
      - _maybe_flush_frame(frame_id): if all REQUIRED_SERVICES have reported for this frame,
        it builds an aggregated payload:

            {
              "frame_id": ...,
              "camera_id": ...,
              "timestamp": ...,
              "extra": {...},
              "detections": [...],
              "captions": <caption text or list>,
              "ocr": <concatenated OCR text>,
              "frame": <base64 image>
            }

      - When complete, publishes this aggregated frame record to topic segmenet and removes
        the frame from memory.

Input / Output:
  - Input topic: aggregate
  - Output topic: segmenet


7. Segment Microservice (SegmenetAgent)
---------------------------------------

File:
  - Core/services/segmenet/SegmenetAgent.py

Purpose:
  - Groups per-frame aggregated analytics into temporal segments per camera.
  - Writes each segment (JSON metadata, summary, and video file) to disk.
  - Sends each completed segment to the Indexr microservice for indexing.

Logical role:
  - This is a standalone microservice that consumes from topic segmenet and owns
    the lifecycle of video segments and segment-level summaries.

Key configuration:

  - Kafka topic consumed: "segmenet"
  - segmenet_time (segment length): 4 seconds (time window per camera).

Internal state:

  - start_frame_time: timestamp of the first frame in the current segment.
  - segmenet: list of aggregated frame dicts belonging to the current segment.

Main operations:

  7.1 add_frame_result(frame_data)
      - For each incoming aggregated frame:
          - Reads frame_data["timestamp"].
          - If start_frame_time is None, sets it to this first timestamp.
          - Computes time difference between start_frame_time and current frame_time.
          - If the time difference is less than segmenet_time:
              - Appends frame_data to self.segmenet and returns (segment still open).
          - If the time difference >= segmenet_time:
              - Closes the current segment:
                  - Decode each frame (base64 -> image) and accumulate in a list.
                  - Remove the raw frame data from the JSON frames before writing to metadata.
                  - Generate a textual summary for the segment (currently via summarize_segment,
                    using an LLM client; the sample code shows a TODO placeholder string but
                    the intended behavior is to call summarize_segment).
                  - Call write_segmenet_to_disk(...) to persist the segment.
                  - Reset start_frame_time and self.segmenet, and return a summary object:
                        {
                          "segment_id": <uuid>,
                          "camera_id": <camera>,
                          "start_time": <start timestamp>,
                          "end_time": <end timestamp>,
                          "summary": <LLM-generated summary>,
                          "segment_path": <filesystem path>
                        }

  7.2 write_segmenet_to_disk(camera_id, segment_data, video_segment, summary)
      - Creates a directory per segment:
            ./data/segments/<camera_id>/<segment_uuid>/
      - Writes:
          - segment.json: the list of per-frame metadata (without the raw frame pixels).
          - summary.txt: the natural-language summary.
          - video.mp4: a reconstructed video from the segment frames using OpenCV.
      - Returns the base path of the segment directory.

  7.3 summarize_segment(segment)
      - Uses an LLM (via LLMClient) to convert a list of frames plus their analytics
        into a single, dense paragraph describing everything that happens in the
        segment in chronological order.
      - The prompt instructs the model not to mention technical terms (detections,
        OCR, scene_graph) and to focus on human-readable narrative.

  7.4 run()
      - Consumes messages from topic segmenet in a loop.
      - For each frame message, calls add_frame_result.
      - When add_frame_result returns a completed segment description, calls the
        Indexr microservice (RealTimeRAG.add_data) to index the segment.

Integration with Indexr:
  - Although the code currently imports RealTimeRAG directly, the logical architecture
    treats Indexr as a separate microservice. In a microservices deployment, the
    segment service would typically call Indexr over HTTP/gRPC or via a dedicated
    Kafka topic instead of importing the class directly.

Input / Output:
  - Input topic: segmenet
  - Side effects:
      - Segment directories written to disk: ./data/segments/<camera_id>/<segment_uuid>/
      - Calls to Indexr to index each segment.


8. Indexr Microservice (RealTimeRAG)
------------------------------------

File:
  - Core/services/indexr/Indexr.py

Purpose:
  - Maintains a vector index of segment summaries and exposes query capabilities.
  - Provides Retrieval-Augmented Generation (RAG) over video segment summaries.

Logical role:
  - Runs as an independent microservice responsible for storage, retrieval, and
    question answering about past video segments.

Key dependencies:

  - SentenceTransformer for embeddings (EMBEDDING_MODEL_NAME from Core/services/config.py).
  - Qdrant (vector database) for persistent storage.
  - Optional in-memory FAISS indices for fast per-user, per-camera search.
  - LLMClient for generating natural-language answers from retrieved contexts.

Core structures:

  - Qdrant collection: QDRANT_COLLECTION (e.g., "video_segments").
  - doc_store: in-memory dictionary of documents keyed by doc_id.
  - faiss_indices: optional FAISS index per (user_id, camera_id) for local search.

Main operations:

  8.1 add_data(user_id, camera_id, segment_path, segment_id, summary, start_time, end_time)
      - Assigns a new doc_id (UUID).
      - Encodes the summary into a vector using SentenceTransformer.
      - Stores document metadata in doc_store:
            {
              "user_id": user_id,
              "camera_id": camera_id,
              "doc_id": doc_id,
              "segment_path": segment_path,
              "segment_id": segment_id,
              "text": summary,
              "start_time": start_time,
              "end_time": end_time
            }
      - Upserts the vector and payload into Qdrant:
            - Collection: QDRANT_COLLECTION
            - Vector size: 384, Distance: COSINE
      - Adds the vector to an in-memory FAISS index for the specific (user_id, camera_id).

  8.2 query(question, user_id, camera_id, top_k=3)
      - Encodes the question into an embedding vector.
      - Queries Qdrant for top_k nearest documents filtered by user_id and camera_id.
      - Builds a list of context documents from the Qdrant payloads.
      - If no contexts are found, returns an empty answer.
      - Otherwise, constructs a combined context string and sends a RAG-style prompt
        to LLMClient.invoke, obtaining a natural-language answer.
      - Returns:
            {
              "answer": <LLM-generated answer>,
              "contexts": [<payloads of matched docs>]
            }

API exposure:
  - In a microservices deployment, RealTimeRAG should be wrapped by a thin API layer
    (REST/gRPC/Websocket) so that other components (e.g., a frontend, a query service,
    or the Segment microservice) can call add_data and query without importing Python
    classes directly.


9. LLMClient
------------

File:
  - Core/LLMClient.py

Purpose:
  - Provides a unified interface to LLMs (Qwen and Groq-based models) with simple
    short-term conversation memory.

Usage in the system:
  - Used by Indexr (RealTimeRAG) to answer questions over retrieved context.
  - Intended to be used by the segment microservice to summarize segments via
    summarize_segment.

Key behavior:
  - Maintains a sliding window of recent system/human/AI messages.
  - Builds LangChain-style message lists and calls either:
      - ChatOpenAI (Qwen backend) via a configurable base URL and API key.
      - ChatGroq via GROQ_API_KEY.
  - Returns plain text content from the model.


10. Shared Utilities
--------------------

File:
  - Core/common/kafka_config.py

  - create_consumer(topic, group_id): returns a KafkaConsumer configured with
    JSON serialization/deserialization and the global bootstrap servers.

  - create_producer(): returns a KafkaProducer configured with JSON serialization
    and string keys.

Other shared modules (not fully detailed here):
  - Core/common/frame_utils.py: base64 encode/decode for frames.
  - Core/common/timeming.py: time difference utilities used by SegmenetAgent.


11. End-to-End Flow Summary
---------------------------

Putting it all together as a pipeline:

  1) Camera ingestion
     - StreamManager + CameraWorker capture frames from one or more cameras.
     - Frames are encoded and published to Kafka topic ingestion.

  2) Normalization
     - IngestionService consumes ingestion and produces normalized messages to analyze.

  3) Parallel analysis
     - DetectionsService, OCRService, and CaptionerService each consume from analyze.
     - Each service performs its model inference and publishes standardized results to aggregate.

  4) Aggregation
     - AggregateService consumes aggregate and waits until all REQUIRED_SERVICES
       (detection, caption, ocr) have reported for a given frame_id.
     - When complete, it publishes an aggregated per-frame record to segmenet.

  5) Segmentation (Segment microservice)
     - SegmenetAgent consumes segmenet and groups frames into time windows per camera.
     - When a window reaches segmenet_time seconds, it finalizes a segment:
         - Writes segment.json, summary.txt, and video.mp4 under ./data/segments/<camera_id>/<segment_uuid>/.
         - Generates a summary using LLMClient (intended behavior).
         - Sends segment metadata (including summary and path) to the Indexr microservice
           by calling add_data.

  6) Indexing and Querying (Indexr microservice)
     - Indexr (RealTimeRAG) stores segment summaries as vectors in Qdrant.
     - A query service or frontend calls Indexr.query(question, user_id, camera_id, top_k)
       to obtain:
         - A natural-language answer about what happened in the relevant segments.
         - The underlying segment metadata and summaries used to answer.

This architecture allows each heavy component (detection, OCR, captioning, aggregation,
segmentation, indexing) to scale independently and be deployed as an isolated microservice.
Kafka topics decouple producers and consumers, and Indexr exposes a RAG interface for
semantic search and question answering over past video segments.


12. Kubernetes Deployment Architecture
--------------------------------------

Target runtime: Kubernetes (K8s).

Each logical service runs as one or more Pods managed by Deployments (or StatefulSets where
state is important). k9s is used as an operational tool for observing and managing pod
health, logs, and scaling.

Core components as K8s workloads:

  - stream-service (Deployment)
      - Pods: N (scaled by number of cameras and input bandwidth).
      - Responsibilities: run StreamManager/CameraWorker instances.
      - Exposes a configuration (ConfigMap/Secret) for camera definitions.

  - ingestion-service (Deployment)
      - Pods: horizontally scalable.
      - Consumes from Kafka topic ingestion and produces to analyze.

  - detections-service (Deployment)
      - Pods: GPU-enabled, horizontally scalable.
      - Each pod runs DetectionsService (YOLO) workers.

  - ocr-service (Deployment)
      - Pods: CPU-heavy; horizontally scalable.
      - Each pod runs OCRService.

  - captioner-service (Deployment)
      - Pods: GPU-enabled; horizontally scalable.
      - Each pod runs CaptionerService (BLIP model).

  - aggregate-service (Deployment)
      - Pods: horizontally scalable.
      - Consumes from aggregate topic and produces to segmenet.

  - segment-service (Deployment)
      - Pods: horizontally scalable.
      - Each pod runs SegmenetAgent, consuming from segmenet topic.

  - indexr-service (Deployment / StatefulSet)
      - Wraps RealTimeRAG with an HTTP/gRPC API.
      - Pods: horizontally scalable behind a K8s Service.
      - Connects to Qdrant cluster (deployed as StatefulSet) and shared storage.

  - chatbot-service (Deployment)
      - New component (see Section 14).
      - Exposes HTTP/WebSocket endpoint for question answering.

  - qdrant-cluster (StatefulSet)
      - Persistent vector database for segment summaries.
      - Uses PersistentVolumeClaims (PVCs) backed by SSD storage.

  - kafka-cluster (outside or inside K8s)
      - Multi-broker Kafka cluster with multiple partitions per topic.

Operational tooling:

  - k9s
      - Used by operators to monitor pods, restarts, resource usage, and rollout status.
      - No code dependency; purely an ops/UI layer on top of Kubernetes.

Autoscaling:

  - Use Horizontal Pod Autoscaler (HPA) for CPU/GPU-intensive services:
      - detections-service, captioner-service, ocr-service, aggregate-service,
        segment-service, indexr-service.
  - Scale based on metrics (CPU, GPU, Kafka consumer lag via custom metrics).


13. Storage Architecture (Frames and Segments)
----------------------------------------------

There are two main categories of storage:

  1) Short-lived frame storage (optional, for replay / debugging).
  2) Long-lived segment storage (video + JSON + summary).

13.1 Frame storage

Current code keeps frames in Kafka messages and in-memory until they are written as part
of segments. For production and high throughput (1000 FPS), consider:

  - Object storage for raw frames (optional):
      - Example: S3/MinIO or a distributed file system.
      - Pattern:
          - Stream service writes raw frames (or batches) to object storage, returns a URI.
          - Kafka messages carry only references (URIs) instead of full base64 images.
      - Benefits:
          - Smaller Kafka messages → higher throughput and lower broker load.
          - Ability to reprocess frames offline (e.g., new models).

13.2 Segment storage

Segments are the main persistent asset and should be stored in durable, scalable storage:

  - Segment layout (logical):
        /segments/<camera_id>/<segment_uuid>/
            segment.json   # per-frame metadata, detections, OCR, captions, etc.
            summary.txt    # human-readable summary
            video.mp4      # stitched video segment

  - In Kubernetes:
      - Use a distributed file system or object storage:
          - Option A: Mount a shared PersistentVolume (e.g., NFS/CSI-backed NAS) to the
            segment-service pods at /segments.
          - Option B (recommended for scale): write segments directly to object storage
            (S3/MinIO) using an SDK; segment_path in Indexr becomes an S3 URI.

  - Indexr payloads store references to segments:
      - segment_path = URI or filesystem path.
      - Qdrant payload mirrors this for later retrieval/playback.


14. Chatbot / Query Service
---------------------------

New logical microservice: chatbot-service.

Purpose:
  - Provide a user-facing API (HTTP/WebSocket) for question answering about stored
    video segments.

Architecture:

  - Deployed as a K8s Deployment behind an Ingress / API Gateway.
  - Stateless pods; all state is in Qdrant + segment storage.

Flow for a user question:

  1) Client (web/mobile) sends a natural-language question plus:
       - user_id
       - camera_id (or set of cameras)
       - optional time filters.

  2) chatbot-service calls indexr-service.query(question, user_id, camera_id, top_k):
       - Indexr queries Qdrant for most relevant segment summaries.
       - RealTimeRAG constructs a context string from retrieved summaries.
       - RealTimeRAG calls LLMClient.invoke to generate an answer.

  3) chatbot-service returns to the client:
       - answer: LLM-generated text.
       - contexts: list of segments (segment_id, summary, start_time, end_time, segment_path).

  4) Optionally, the client can use segment_path to fetch and play the underlying
     video segments (via a separate media service/CDN).

Scaling:
  - chatbot-service and indexr-service can scale horizontally; they are stateless
    (aside from Qdrant and any cache layer).
  - For heavy load, you can:
       - Add a caching layer (Redis) for frequent queries.
       - Use a separate low-latency LLM endpoint for short answers.


15. Architecture Weaknesses and Bottlenecks
-------------------------------------------

This section highlights weaknesses in the current design and how they affect scalability,
especially toward the 1000 FPS target.

15.1 Large frames in Kafka messages

  - Problem:
      - Frames are sent as base64-encoded JPEGs in Kafka messages across multiple
        topics (ingestion, analyze, aggregate, segmenet).
      - This inflates message size and bandwidth and increases broker and network load.
  - Impact:
      - Kafka throughput may become the main bottleneck well before 1000 FPS.
  - Mitigation:
      - Move to reference-based messages:
          - Store frames/segments in object storage; Kafka messages carry URIs.
          - Use smaller image sizes or stronger compression for real-time analytics.

15.2 Tight coupling between segment-service and Indexr implementation

  - Problem:
      - SegmenetAgent currently imports RealTimeRAG directly and calls add_data in-process.
  - Impact:
      - Harder to deploy segment-service and indexr-service as truly independent microservices.
      - Harder to scale them independently or use different languages/runtimes.
  - Mitigation:
      - Expose Indexr as a network service (HTTP/gRPC) and have segment-service call
        it over the network or send indexing jobs via a dedicated Kafka topic.

15.3 Single aggregate Service instance (logical)

  - Problem:
      - The aggregation logic keeps per-frame state in memory in each AggregateService
        instance. If not carefully partitioned by Kafka key (frame_id), multiple pods
        could see partial state for the same frame.
  - Impact:
      - Risk of incomplete aggregation or duplicated work at high throughput.
  - Mitigation:
      - Ensure Kafka partitioning by frame_id and use consumer groups with one consumer
        per partition.
      - Design AggregateService to be safely horizontally scalable based on partitions.

15.4 CPU/GPU hot spots

  - Detections and captioning are GPU-heavy; OCR is CPU-heavy.
  - Without autoscaling and proper GPU scheduling, a single GPU node can be saturated.

15.5 Single Qdrant / LLM instance

  - Problem:
      - If Qdrant runs as a single-node instance and LLMClient connects to a single
        model endpoint, these become central bottlenecks.
  - Mitigation:
      - Run Qdrant as a cluster with sharding/replication.
      - Use multiple LLM replicas behind a load balancer and/or a managed LLM service.

15.6 Local disk segment storage

  - Problem:
      - Writing segments to a single node's local disk does not scale across the
        cluster and is a single point of failure.
  - Mitigation:
      - Move to shared or object storage as described in Section 13.


16. Scaling to 1000 Frames per Second (FPS)
-------------------------------------------

To support ~1000 FPS end-to-end, you need to design for horizontal scalability and
minimize per-frame overhead.

Key recommendations:

16.1 Kafka and message design

  - Use a multi-broker Kafka cluster.
  - Create sufficient partitions for each high-throughput topic (ingestion, analyze,
    aggregate, segmenet). For 1000 FPS, start with 16–32 partitions per topic and
    adjust based on benchmarks.
  - Partition by (camera_id, frame_id) to keep all messages for a frame on the same
    partition (important for aggregation and segmentation).
  - Avoid storing full-resolution images in Kafka. Prefer:
      - Compressed thumbnails for analytics.
      - References (URIs) to full frames in object storage.

16.2 Horizontal scaling of analyzer services

  - DetectionsService (YOLO):
      - Scale GPU-enabled pods based on Kafka consumer lag and GPU utilization.
      - Consider batching frames per inference call for better GPU efficiency.

  - CaptionerService (BLIP):
      - Similar to detections; batch frames and tune model size (large vs base).

  - OCRService (EasyOCR):
      - Run multiple CPU pods; consider switching to more optimized OCR models if needed.

16.3 Aggregation and segmentation

  - Ensure AggregateService is stateless per frame_id and only holds short-lived
    state, partitioned via Kafka keys.
  - For segmentation:
      - Consider per-camera partitions or per-camera segment workers to avoid
        cross-camera contention.
      - Use timers or watermarking logic to close segments in case of dropped frames.

16.4 Storage throughput

  - Use high-IOPS SSD-backed storage for Qdrant and Kafka.
  - For segments:
      - Write video.mp4 and JSON in a streaming fashion, avoid huge bursts.
      - Use asynchronous writes where possible (e.g., write segments off the main
        critical path of frame ingestion using background workers).

16.5 Indexr and chatbot scaling

  - Run multiple indexr-service replicas behind a K8s Service.
  - For heavy query load:
      - Enable Qdrant sharding and replication.
      - Add caching of recent query results and popular segments.
  - For LLM calls:
      - Use smaller/faster models for interactive chatbot responses.
      - Introduce a rate limiter and queueing system for queries.

16.6 Observability and tuning

  - Metrics to track:
      - Kafka lag per consumer group and partition.
      - Per-service latency (ingestion → analysis → aggregation → segmentation → indexing).
      - GPU/CPU utilization and memory usage per service pod.
      - Qdrant query latency and LLM latency.
  - Use Prometheus + Grafana (or similar) for metrics and dashboards.
  - Use k9s to quickly inspect pod states, restarts, and resource pressure.

By applying these enhancements—especially optimizing Kafka payloads, using
reference-based storage, horizontally scaling analysis services, and clustering
Kafka/Qdrant/LLM backends—the architecture can be evolved to support on the order
of 1000 frames per second in a robust, observable, and maintainable way on Kubernetes.


17. High-Level Flow Diagram (ASCII)
-----------------------------------

Below is an ASCII-style flow diagram of the full system, including Kubernetes, storage,
indexing, and the chatbot.

  +------------------+              +---------------------------+
  |   Cameras (N)    |              |     K8s Cluster           |
  |  RTSP / Files    |              |  (monitored via k9s)      |
  +---------+--------+              +-------------+-------------+
            |                                     |
            | Frames (RTSP / video)              |
            v                                     |
  +-------------------------+                     |
  |  stream-service (Pods)  |                     |
  |  StreamManager/Workers  |                     |
  +------------+------------+                     |
               | 1) encode frame (JPEG/base64 or store to object storage)
               | 2) optionally write raw frame to object storage (S3/MinIO)
               |    and send only URI in message
               v
       Kafka topic: ingestion  (multi-broker, many partitions)
               |
               v
  +---------------------------+
  | ingestion-service (Pods)  |
  | normalize messages        |
  +------------+--------------+
               |
               v
       Kafka topic: analyze
               |
      +--------+--------+-------------------+
      |                 |                   |
      v                 v                   v
+-------------+   +-------------+    +--------------+
| detections- |   | captioner-  |    |  ocr-service |
| service     |   | service     |    |  (Pods)      |
| (GPU Pods)  |   | (GPU Pods)  |    |  (CPU Pods)  |
+------+------+   +------+------+    +------+-------+
       |                 |                  |
       v                 v                  v
         Kafka topic: aggregate (per-frame results from all analyzers)
                            |
                            v
                 +------------------------+
                 |  aggregate-service     |
                 |  (Pods)               |
                 +-----------+------------+
                             |
                             v
                   Kafka topic: segmenet
                             |
                             v
                 +------------------------+
                 |  segment-service       |
                 |  SegmenetAgent (Pods) |
                 +-----------+------------+
                             |
                             | 1) group frames into segments per camera
                             | 2) decode frames & write video.mp4
                             | 3) write segment.json & summary.txt
                             v
                 +-----------------------------------------------+
                 |  Segment Storage                              |
                 |  /segments/<camera>/<segment_uuid>/          |
                 |   - segment.json                             |
                 |   - summary.txt                              |
                 |   - video.mp4                                |
                 |  (shared PV or object storage: S3/MinIO)     |
                 +----------------------+------------------------+
                                        |
                                        | segment_path + summary + metadata
                                        v
                              +--------------------------+
                              |   indexr-service (Pods)  |
                              |   RealTimeRAG + LLM      |
                              +-----------+--------------+
                                          |
                                          |  store embeddings + payload
                                          v
                               +------------------------------+
                               |  Qdrant Cluster (Stateful)   |
                               |  Vector DB (SSD-backed PVCs) |
                               +------------------------------+


User query path (chatbot):

  +--------------------+         HTTPS/WebSocket        +----------------------+
  |  Web / Mobile UI   | -----------------------------> |  chatbot-service     |
  +--------------------+                                |  (Pods, Ingress)    |
                                                        +----------+-----------+
                                                                   |
                                                                   | question, user_id, camera_id
                                                                   v
                                                         +---------+----------+
                                                         |  indexr-service   |
                                                         |  RealTimeRAG      |
                                                         +---------+---------+
                                                                   |
                                                                   | vector search
                                                                   v
                                                         +---------+---------+
                                                         |  Qdrant Cluster   |
                                                         +---------+---------+
                                                                   |
                                                                   | top-K segments (summaries,
                                                                   | start/end, segment_path)
                                                                   v
                                                         +---------+---------+
                                                         |  RealTimeRAG      |
                                                         |  + LLMClient      |
                                                         +---------+---------+
                                                                   |
                                                                   | final natural-language answer
                                                                   v
                                                        +----------------------+
                                                        |  chatbot-service     |
                                                        +----------+-----------+
                                                                   |
                                                                   v
                                                        +----------------------+
                                                        |   Web / Mobile UI    |
                                                        +----------------------+

This diagram shows the main data paths:

  - Ingestion/processing path: Cameras → stream-service → Kafka → analyzers → aggregate →
    segment-service → segment storage + indexr-service → Qdrant.
  - Query path: UI → chatbot-service → indexr-service → Qdrant → LLM → chatbot-service → UI.

Kafka, Qdrant, and all stateless services are expected to scale horizontally on Kubernetes
with observability and operations handled via Prometheus/Grafana and k9s.
